{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Analysis of biases of the NC measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from numpy.linalg import pinv\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Let's first implement the NC measure as:\n",
    "$$\n",
    "\\Sigma_w = \\frac{1}{n} \\sum\\limits_{j=1}^K \\sum\\limits_{y_i = j} (x_i - \\mu_j) \\cdot (x_i - \\mu_j)^T\n",
    "$$\n",
    "$$\n",
    "\\Sigma_b = \\frac{1}{K} \\sum\\limits_{j=1}^K (\\mu_j - \\mu)(\\mu_j - \\mu)^T\n",
    "$$\n",
    "$$\n",
    "NC = \\frac{1}{K} \\text{trace}(\\Sigma_w \\cdot \\Sigma_b^{+})\n",
    "$$\n",
    "This should match with the definition https://www.pnas.org/doi/full/10.1073/pnas.2015509117.\n",
    "\n",
    "However, it is unclear whether the within-class covariance $\\Sigma_w$, should be rather normalized for each class to compensate\n",
    "for classes with quite a lot of examples: \n",
    "$$\n",
    "\\Sigma_w = \\frac{1}{K} \\sum\\limits_{j=1}^K \\frac{1}{n_j} \\sum\\limits_{i=1}^{n_j} (x_i - \\mu_j) \\cdot (x_i - \\mu_j)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NC1_byhand(X, y):\n",
    "    \"\"\" NC computation without using the covariance operator \"\"\"\n",
    "    # Deprecated version which can be ignored\n",
    "    nclasses = len(np.unique(y))\n",
    "    ohe_y = np.eye(nclasses)[y]\n",
    "    n_per_class = np.sum(ohe_y, axis=0)\n",
    "    sum_per_class = (X.T @ ohe_y)\n",
    "    mean_per_class = sum_per_class / n_per_class\n",
    "    X_class_norm = X - ohe_y @ mean_per_class.T\n",
    "    n_per_class_examples = ohe_y @ n_per_class\n",
    "    X_class_norm_balanced = X_class_norm / np.sqrt(n_per_class_examples)[:, None]\n",
    "    SigmaW = (1.0/nclasses) * X_class_norm_balanced.T @ X_class_norm_balanced\n",
    "    \n",
    "    \n",
    "    mean = np.mean(X, axis=0)\n",
    "    mean_per_class_norm = mean_per_class - mean[:, None] \n",
    "    SigmaB = (1.0/nclasses) * mean_per_class_norm @ mean_per_class_norm.T \n",
    "\n",
    "    NC_value = np.trace(SigmaW @ pinv(SigmaB))/nclasses\n",
    "\n",
    "    return NC_value, nclasses\n",
    "\n",
    "\n",
    "def NC1(X, y, \n",
    "        inversion_method=\"pinv\", \n",
    "        return_matrices=False,\n",
    "        normalization=\"overall\"):\n",
    "    \"\"\" Faster version using the covariance operator \n",
    "    @param inversion_method \"pinv\", \"eig\"\n",
    "    @param normalization \"overall\" or \"class\" \n",
    "    \"\"\"\n",
    "    classes = np.unique(y)\n",
    "    nclasses = len(classes)\n",
    "    dim = X.shape[1]\n",
    "    classmeans = []\n",
    "    SigmaW = np.zeros((dim, dim))\n",
    "    for i in classes:\n",
    "        class_examples = X[y==i, :].T\n",
    "        C = np.cov(class_examples, bias=True)\n",
    "        if normalization==\"class\":\n",
    "            SigmaW += C\n",
    "        else:\n",
    "            SigmaW += C * class_examples.shape[1]\n",
    "        classmean = np.mean(class_examples, axis=1)\n",
    "        classmeans.append(classmean)\n",
    "\n",
    "    if normalization==\"class\":\n",
    "        SigmaW /= nclasses\n",
    "    else:\n",
    "        SigmaW /= X.shape[0]\n",
    "    \n",
    "    classmeans = np.array(classmeans).T\n",
    "    SigmaB = np.cov(classmeans, bias=True).reshape(dim,dim) # reshaping for the one-dimensional case\n",
    "\n",
    "    if inversion_method==\"pinv\":\n",
    "        NC_value = np.trace(SigmaW @ pinv(SigmaB)) / nclasses\n",
    "    elif inversion_method==\"eig\":\n",
    "        e, eigv = np.linalg.eig(SigmaB)\n",
    "        # this is ignored in https://github.com/rhubarbwu/neural-collapse/blob/main/neural_collapse/measure.py (L48)\n",
    "        nonzero_e = np.abs(e)>1e-8 \n",
    "        e[nonzero_e] = 1.0 / e[nonzero_e]\n",
    "        NC_value = np.real(np.trace(SigmaW @ eigv @ np.diag(e) @ eigv.T) / nclasses)\n",
    "\n",
    "    if return_matrices:\n",
    "        return NC_value, nclasses, SigmaB, SigmaW\n",
    "    else:\n",
    "        return NC_value, nclasses\n",
    "\n",
    "\n",
    "def MeanDistanceRatio(X, y):\n",
    "    \"\"\" Blue lines of Fig. 2 in https://www.pnas.org/doi/full/10.1073/pnas.2015509117 \"\"\"\n",
    "    classes = np.unique(y)\n",
    "    nclasses = len(classes)\n",
    "    dim = X.shape[1]\n",
    "    mean_distances = []\n",
    "    global_mean = np.mean(X)\n",
    "    for i in classes:\n",
    "        class_examples = X[y==i, :].T\n",
    "        classmean = np.mean(class_examples, axis=1)\n",
    "        mean_distances.append(np.linalg.norm(classmean-global_mean))\n",
    "\n",
    "    return np.std(mean_distances)/np.mean(mean_distances), nclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCMeasure = MeanDistanceRatio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Analyzing all scikit learn datasets for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ lambda: datasets.load_iris(),\n",
    "         lambda: datasets.load_digits(),\n",
    "         lambda: datasets.load_wine(),\n",
    "         lambda: datasets.load_breast_cancer() ]\n",
    "\n",
    "for dataset in data:\n",
    "    D = dataset()\n",
    "    X, y = D.data, D.target\n",
    "    NC_value1, nclasses = NCMeasure(X, y)\n",
    "\n",
    "    print (f\"{nclasses:5d} {NC_value1:8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Sample classes from the digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = datasets.load_digits()\n",
    "X, y = D.data, D.target\n",
    "class_idx = np.unique(y)\n",
    "max_samples = 20\n",
    "rng = np.random.default_rng()\n",
    "class_range = range(2, 10)\n",
    "NC = {}\n",
    "for c in class_range:\n",
    "    NC_cs = []\n",
    "    for i in range(max_samples):\n",
    "        s = rng.choice(class_idx, size=c, replace=False, shuffle=True)\n",
    "        X_s = X[np.in1d(y, s)]\n",
    "        y_s = y[np.in1d(y, s)]\n",
    "        NC_c, _ = NCMeasure(X_s, y_s)\n",
    "        NC_cs.append(NC_c)\n",
    "        \n",
    "    NC[c] = NC_cs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(class_range, \n",
    "             [ np.mean(NC[c]) for c in class_range ],\n",
    "             [ np.std(NC[c]) for c in class_range ],\n",
    "            )\n",
    "plt.xlabel(\"number of classes\")\n",
    "plt.ylabel(\"NC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Experiments with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gaussian_classification(n_samples, n_features, n_classes, n_clusters_per_class):\n",
    "    # choose global mean\n",
    "    # 1. mu sampled from N(0,I)\n",
    "    global_mean = np.random.randn(n_features)\n",
    "    global_cov = np.eye(n_features)\n",
    "    X = []\n",
    "    y = []\n",
    "    # 2. mu_j sampled from N(mu, I), i.e. from N(0, I)\n",
    "    class_means = np.random.multivariate_normal(global_mean, global_cov, size=n_classes)\n",
    "    for i in range(n_classes):\n",
    "        class_mean = class_means[i,:]\n",
    "        # 3. cluster_{j,l} sampled from N(mu_j, 0.1*I)\n",
    "        cluster_means = np.random.multivariate_normal(class_mean, global_cov*0.1, size=n_clusters_per_class)\n",
    "        for k in range(n_clusters_per_class):\n",
    "            cluster_mean = cluster_means[k,:]\n",
    "            n_examples = n_samples // (n_classes * n_clusters_per_class)\n",
    "            # 4. x sampled from N(cluster_{j,l}, 0.01*I)\n",
    "            examples = np.random.multivariate_normal(cluster_mean, global_cov*0.01, size=n_examples)\n",
    "            X.append(examples)\n",
    "            y.append(np.ones(n_examples)*i)\n",
    "\n",
    "    return np.vstack(X), np.hstack(y).astype(int)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_gaussian_classification(n_samples=128, n_features=2, n_classes=4, n_clusters_per_class=2)\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 32\n",
    "nclasses_range = np.arange(2,2*D)\n",
    "\n",
    "generation_methods = {\"sklearn\": lambda n_classes: datasets.make_classification(\n",
    "                                        n_samples=1024, \n",
    "                                        n_features=D, \n",
    "                                        n_informative=D, \n",
    "                                        n_redundant=0, \n",
    "                                        n_repeated=0, \n",
    "                                        n_classes=n_classes, \n",
    "                                        n_clusters_per_class=1),\n",
    "                      \"gaussian\": lambda n_classes: make_gaussian_classification(n_samples=1024, \n",
    "                                                                   n_features=D, \n",
    "                                                                   n_classes=n_classes, \n",
    "                                                                   n_clusters_per_class=1)}\n",
    "\n",
    "generation_method = generation_methods[\"gaussian\"]\n",
    "\n",
    "NC_values = []\n",
    "nclasses_values = [] \n",
    "\n",
    "for k in nclasses_range:\n",
    "    X, y = generation_method(k)\n",
    "    NC, nclasses = NCMeasure(X, y)\n",
    "    NC_values.append(NC)\n",
    "    nclasses_values.append(nclasses)\n",
    "\n",
    "nclasses_values = np.array(nclasses_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nclasses_values, NC_values)\n",
    "plt.xlabel(\"Number of synthetic classes\")\n",
    "plt.ylabel(\"NC value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
